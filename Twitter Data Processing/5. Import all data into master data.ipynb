{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "\n",
    "from datetime import datetime, date\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_RANGES =  pd.date_range(datetime(2012, 1, 1), datetime(2012, 12,31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DV : Lottery Ticket Sales: \n",
    "##### We acquired daily lottery purchases, by postal code, for the years 2012 (exploratory dataset) and 2013 (confirmatory dataset) from the NYS Gaming Commission using a Freedom Of Information Act (FOIA) request. We aggregated daily lottery ticket sales, across 174 postal Codes, for all daily, non-jackpot-based lottery games available in New York State.\n",
    "##### Because jackpot amounts are not publicly disclosed before daily drawings, and winning odds remain constant (in all of these games, prizes are awarded to players whose chosen numbers match the drawn numbers regardless of the number of winning players), the expected values of each of these games (payoff × probability of winning) remain constant over days. For each postal code, we summed the sales of these games and divided this composite by the postal code’s adult population to control for population differences across postal codes yielding a measure of per capita purchases per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lottery ticket sales \n",
    "lottery_12 = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Otto data\\nyc_2012_lottery.csv\", index_col = False)\n",
    "# Select only the SalesDate, Zip and per capita columns\n",
    "lottery_12 = lottery_12[[\"SalesDate\", \"ZIP\", 'composite_per_capita']]\n",
    "# Change the \"created_at\" column to type date\n",
    "lottery_12[\"SalesDate\"] = pd.to_datetime(lottery_12[\"SalesDate\"]).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lottery_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any missing dates with the mean sales per capita for that zip code\n",
    "zip_codes = list(set(lottery_12[\"ZIP\"]))\n",
    "\n",
    "end_2012 = date(2012,12,31)\n",
    "\n",
    "dates_in_2012 = [end_2012 - timedelta(days=x) for x in range(366)] \n",
    "\n",
    "for zip_code in zip_codes:\n",
    "    # Find the rows in the lottery_data fr\n",
    "    lottery_rows = lottery_12[lottery_12[\"ZIP\"] == zip_code]\n",
    "    \n",
    "    # If the length of the zip codes rows is 366 its ok\n",
    "    if len(lottery_rows) == 366:\n",
    "        pass\n",
    "    # If it is not 366, it has missing data\n",
    "    else:\n",
    "        # Check if nan\n",
    "        #print(lottery_rows['composite_per_capita'].isnull().values.any())\n",
    "        \n",
    "        \n",
    "        # Get the mean of the per capita score\n",
    "        mean_per_capita_sales = lottery_rows['composite_per_capita'].mean()\n",
    "        \n",
    "        # Loop through date - if there is a missing value replace it with mean\n",
    "        for date in dates_in_2012:\n",
    "            \n",
    "            if date not in list(lottery_rows[\"SalesDate\"]):\n",
    "                \n",
    "                # Update the main lottery 12 dataframe\n",
    "                #new_row = {\"SalesDate\": date, \"ZIP\" : zip_code, 'composite_per_capita': mean_per_capita_sales}\n",
    "                #print(new_row)\n",
    "                #lottery_12.append(new_row, ignore_index = True)\n",
    "                lottery_12.loc[len(lottery_12.index)] = [date, zip_code, mean_per_capita_sales]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lottery_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  DV : Sports game prediction errors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sports team prediction errors\n",
    "sports_pe_12 = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Otto data\\Sports\\nyc_metro_sports_2012.csv\")\n",
    "# sports_pe_13 = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Otto data\\Sports\\nyc_sports_2013.csv\")\n",
    "\n",
    "# Select the data and prediction error columns\n",
    "sports_cols = [\"date\", 'knicks_pe','mets_pe','yankees_pe','giants_pe','devils_pe', 'sum_pe']\n",
    "sports_pe_12 = sports_pe_12[sports_cols] \n",
    "# sports_pe_13 = sports_pe_2013[sports_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV : VADER PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the VADER pe scores for each team and both years\n",
    "vader_pe_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\vader_pe_scores\"\n",
    "\n",
    "devils_12_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2012, \"devils\"), index_col = 0)\n",
    "giants_12_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2012, \"giants\"), index_col = 0)\n",
    "knicks_12_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2012, \"knicks\"), index_col = 0)\n",
    "mets_12_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2012, \"mets\"), index_col = 0)\n",
    "yankees_12_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2012, \"yankees\"), index_col = 0)\n",
    "\n",
    "#devils_13_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2013, \"devils\"), index_col = 0)\n",
    "#giants_13_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2013, \"giants\"), index_col = 0)\n",
    "#knicks_13_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2013, \"knicks\"), index_col = 0)\n",
    "#mets_13_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2013, \"mets\"), index_col = 0)\n",
    "#yankees_13_vader = pd.read_csv(vader_pe_dir + \"\\{}_{}_vader_pe.csv\".format(2013, \"yankees\"), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV : Pysent PE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysent_pe_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\pysent_pe_scores\"\n",
    "\n",
    "devils_12_pysent = pd.read_csv(pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"devils\"), index_col = 0)\n",
    "giants_12_pysent = pd.read_csv(pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"giants\"), index_col = 0)\n",
    "knicks_12_pysent = pd.read_csv(pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"knicks\"), index_col = 0)\n",
    "mets_12_pysent = pd.read_csv(pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"mets\"), index_col = 0)\n",
    "yankees_12_pysent = pd.read_csv(pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"yankees\"), index_col = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV: Roberta 'suprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_pe_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\roberta_pe_scores\"\n",
    "\n",
    "devils_12_roberta = pd.read_csv(roberta_pe_dir + \"\\{}_{}_roberta_pe.csv\".format(2012, \"devils\"), index_col = 0)\n",
    "giants_12_roberta = pd.read_csv(roberta_pe_dir + \"\\{}_{}_roberta_pe.csv\".format(2012, \"giants\"), index_col = 0)\n",
    "knicks_12_roberta = pd.read_csv(roberta_pe_dir + \"\\{}_{}_roberta_pe.csv\".format(2012, \"knicks\"), index_col = 0)\n",
    "mets_12_roberta = pd.read_csv(roberta_pe_dir + \"\\{}_{}_roberta_pe.csv\".format(2012, \"mets\"), index_col = 0)\n",
    "yankees_12_roberta = pd.read_csv(roberta_pe_dir + \"\\{}_{}_roberta_pe.csv\".format(2012, \"yankees\"), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV: Distilbert 'surprise' - note taking the mean surprise, not the prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_pe_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\distilbert_pe_scores\"\n",
    "\n",
    "devils_12_distilbert = pd.read_csv(distilbert_pe_dir + \"\\{}_{}_distilbert_pe.csv\".format(2012, \"devils\"), index_col = 0)\n",
    "giants_12_distilbert = pd.read_csv(distilbert_pe_dir + \"\\{}_{}_distilbert_pe.csv\".format(2012, \"giants\"), index_col = 0)\n",
    "knicks_12_distilbert = pd.read_csv(distilbert_pe_dir + \"\\{}_{}_distilbert_pe.csv\".format(2012, \"knicks\"), index_col = 0)\n",
    "mets_12_distilbert = pd.read_csv(distilbert_pe_dir + \"\\{}_{}_distilbert_pe.csv\".format(2012, \"mets\"), index_col = 0)\n",
    "yankees_12_distilbert = pd.read_csv(distilbert_pe_dir + \"\\{}_{}_distilbert_pe.csv\".format(2012, \"yankees\"), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV: Not negatives - pysent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neg_pysent_pe_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\pysent_scores_no_negatives\"\n",
    "\n",
    "devils_12_no_neg_pysent = pd.read_csv(no_neg_pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"devils\"), index_col = 0)\n",
    "giants_12_no_neg_pysent = pd.read_csv(no_neg_pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"giants\"), index_col = 0)\n",
    "knicks_12_no_neg_pysent = pd.read_csv(no_neg_pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"knicks\"), index_col = 0)\n",
    "mets_12_no_neg_pysent = pd.read_csv(no_neg_pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"mets\"), index_col = 0)\n",
    "yankees_12_no_neg_pysent = pd.read_csv(no_neg_pysent_pe_dir + \"\\{}_{}_pysent_pe.csv\".format(2012, \"yankees\"), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.DataFrame(columns = [\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dates from a random dataframe\n",
    "dates = list(devils_12_vader[\"date\"])\n",
    "master_data[\"date\"] = dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add lottery ticket sales to master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for each sales zip code in master_data\n",
    "# Loop through each ZIP code\n",
    "zip_codes = list(set(lottery_12[\"ZIP\"]))\n",
    "\n",
    "zip_missing_data = {}\n",
    "\n",
    "for zip_code in zip_codes:\n",
    "    \n",
    "    # Filter lottery_sales according to ZIP\n",
    "    # zip_sales =  lottery_sales[lottery_sales[\"ZIP\"] == zip_code]\n",
    "    zip_sales =  lottery_12[lottery_12[\"ZIP\"] == zip_code]\n",
    "    #print(list(zip_sales[\"composite_per_capita\"]))\n",
    "    \n",
    "    if len(zip_sales) == 366:\n",
    "        # Create a new column using the composite_per_capita sales metric\n",
    "        col_name = \"sales_{}\".format(zip_code)\n",
    "        master_data[col_name] = list(zip_sales[\"composite_per_capita\"])\n",
    "        \n",
    "    else:\n",
    "        zip_missing_data[zip_code] = len(zip_sales)\n",
    "    #print(len(list(zip_sales[\"composite_per_capita\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an overall composite_per_capita for the whole New York area\n",
    "sales_data = master_data[list(master_data.columns[master_data.columns.str.contains(\"sales_\")])]\n",
    "sales_data[\"NYC_sales\"] = master_data.mean(axis=1)\n",
    "\n",
    "# assign overall sales column to master_data\n",
    "master_data[\"sales_NYC\"] = sales_data[\"NYC_sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Borough for each ZIP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borough has \"Borough\", \"Neighborhood\" and \"ZIP\"\n",
    "zip_names = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Otto data\\2012_paper\\zip_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in a borough for each ZIP code in the origal lottery sales file\n",
    "lottery_12[\"borough\"] = \"\"\n",
    "\n",
    "for zip_code in zip_codes:\n",
    "    lottery_12.loc[lottery_12[\"ZIP\"] == zip_code, \"borough\"] = list(zip_names.loc[zip_names[\"ZIP\"] ==  zip_code,\"Borough\"])[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite_per_capita sales figure in master_data for each borough on each day\n",
    "boroughs = list(set(zip_names[\"Borough\"]))\n",
    "\n",
    "\n",
    "# Select by borough\n",
    "for borough in boroughs:\n",
    "    per_capita_sales = []\n",
    "    # For each date find the mean composite_per_capita\n",
    "    for date in DATE_RANGES:\n",
    "        # select rows of lottery_12 that relate to the borough and date\n",
    "        borough_rows = lottery_12.loc[lottery_12[\"borough\"] == borough]\n",
    "        date_rows = borough_rows.loc[borough_rows[\"SalesDate\"] == date]\n",
    "        # Calculate the mean for the borough on each day\n",
    "        mean_borough_on_that_date = np.mean(date_rows[\"composite_per_capita\"])\n",
    "        per_capita_sales.append(mean_borough_on_that_date)\n",
    "    \n",
    "    # Update master_data\n",
    "    col_name = \"sales_{}\".format(borough)\n",
    "    master_data[col_name] = per_capita_sales\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Sports Fixture Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate the sports fixtures onto the master data dataframe. \n",
    "sports_pe_12  = sports_pe_12[[\"knicks_pe\",\"mets_pe\",\"yankees_pe\",\"giants_pe\",\"devils_pe\",\"sum_pe\"]]\n",
    "master_data = pd.concat([master_data,sports_pe_12] , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Vader pe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VADER SCORES\n",
    "# iterate through each of the vader dataframes and create a new col for each team\n",
    "vader_dataframes = {\"devils\": [devils_12_vader], \n",
    "                    \"giants\": [giants_12_vader],\n",
    "                    \"knicks\": [knicks_12_vader], \n",
    "                    \"mets\": [mets_12_vader],\n",
    "                    \"yankees\": [yankees_12_vader]}\n",
    "\n",
    "for team in vader_dataframes.keys():    \n",
    "    # Add the vader pe column to master_data\n",
    "    col_name = team + \"_vader_pe\"\n",
    "    master_data[col_name] = vader_dataframes[team][0][\"PE_Vader\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_data[master_data[\"knicks_vader_pe\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add pysent pe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pysent SCORES\n",
    "# iterate through each of the pysent dataframes and create a new col for each team\n",
    "pysent_dataframes = {\"devils\": [devils_12_pysent], \n",
    "                    \"giants\": [giants_12_pysent],\n",
    "                    \"knicks\": [knicks_12_pysent], \n",
    "                    \"mets\": [mets_12_pysent],\n",
    "                    \"yankees\": [yankees_12_pysent]}\n",
    "\n",
    "for team in pysent_dataframes.keys():    \n",
    "    # Add the pysent pe column to master_data\n",
    "    col_name = team + \"_pysent_pe\"\n",
    "    master_data[col_name] = pysent_dataframes[team][0][\"PE_pysent\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Roberta pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### roberta SCORES\n",
    "# iterate through each of the roberta dataframes and create a new col for each team\n",
    "roberta_dataframes = {\"devils\": [devils_12_roberta], \n",
    "                    \"giants\": [giants_12_roberta],\n",
    "                    \"knicks\": [knicks_12_roberta], \n",
    "                    \"mets\": [mets_12_roberta],\n",
    "                    \"yankees\": [yankees_12_roberta]}\n",
    "\n",
    "for team in roberta_dataframes.keys():    \n",
    "    # Add the roberta pe column to master_data\n",
    "    col_name = team + \"_roberta_pe\"\n",
    "    master_data[col_name] = roberta_dataframes[team][0][\"PE_roberta\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add mean surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in roberta_dataframes.keys(): \n",
    "    # Add the mean roberta scores to master_data\n",
    "    col_name = team + \"_mean_roberta\"\n",
    "    master_data[col_name] = roberta_dataframes[team][0][\"mean_roberta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add distiberta mean surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distilbert SCORES\n",
    "# iterate through each of the distilbert dataframes and create a new col for each team\n",
    "no_neg_dataframes = {\"devils\": [devils_12_distilbert], \n",
    "                    \"giants\": [giants_12_distilbert],\n",
    "                    \"knicks\": [knicks_12_distilbert], \n",
    "                    \"mets\": [mets_12_distilbert],\n",
    "                    \"yankees\": [yankees_12_distilbert]}\n",
    "\n",
    "for team in no_neg_dataframes.keys(): \n",
    "    # Add the mean distilbert scores to master_data\n",
    "    col_name = team + \"_mean_distilbert\"\n",
    "    master_data[col_name] = no_neg_dataframes[team][0][\"mean_distilbert\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add no negative pysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no_neg_pysent SCORES\n",
    "# iterate through each of the no_neg_pysent dataframes and create a new col for each team\n",
    "no_neg_pysent_dataframes = {\"devils\": [devils_12_no_neg_pysent], \n",
    "                    \"giants\": [giants_12_no_neg_pysent],\n",
    "                    \"knicks\": [knicks_12_no_neg_pysent], \n",
    "                    \"mets\": [mets_12_no_neg_pysent],\n",
    "                    \"yankees\": [yankees_12_no_neg_pysent]}\n",
    "\n",
    "for team in no_neg_pysent_dataframes.keys(): \n",
    "    # Add the mean no_neg_pysent scores to master_data\n",
    "    col_name = team + \"_mean_no_neg_pysent\"\n",
    "    master_data[col_name] = no_neg_pysent_dataframes[team][0][\"PE_pysent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make log sales figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clark\\miniconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:274: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "sales_columns = master_data.columns[master_data.columns.str.contains(\"sales_\")]\n",
    "# Create a new column with the log of each sales column\n",
    "for col in sales_columns:\n",
    "    # Take the log of each value\n",
    "    log_sales = log(master_data[col])\n",
    "    \n",
    "    # Update master_data\n",
    "    col_name = \"log_{}\".format(col)\n",
    "    master_data[col_name] = log_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales_10280</th>\n",
       "      <th>sales_10301</th>\n",
       "      <th>sales_10302</th>\n",
       "      <th>sales_10303</th>\n",
       "      <th>sales_10304</th>\n",
       "      <th>sales_10305</th>\n",
       "      <th>sales_10306</th>\n",
       "      <th>sales_10307</th>\n",
       "      <th>sales_10308</th>\n",
       "      <th>...</th>\n",
       "      <th>log_sales_11236</th>\n",
       "      <th>log_sales_11237</th>\n",
       "      <th>log_sales_11238</th>\n",
       "      <th>log_sales_11239</th>\n",
       "      <th>log_sales_NYC</th>\n",
       "      <th>log_sales_Staten Island</th>\n",
       "      <th>log_sales_Queens</th>\n",
       "      <th>log_sales_Bronx</th>\n",
       "      <th>log_sales_Manhattan</th>\n",
       "      <th>log_sales_Brooklyn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>0.064536</td>\n",
       "      <td>0.430190</td>\n",
       "      <td>0.939558</td>\n",
       "      <td>0.916459</td>\n",
       "      <td>0.337124</td>\n",
       "      <td>0.541060</td>\n",
       "      <td>0.522432</td>\n",
       "      <td>0.687009</td>\n",
       "      <td>0.330482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318321</td>\n",
       "      <td>-0.968899</td>\n",
       "      <td>-0.600332</td>\n",
       "      <td>-0.788025</td>\n",
       "      <td>-0.864180</td>\n",
       "      <td>-0.591675</td>\n",
       "      <td>-1.043726</td>\n",
       "      <td>-0.675271</td>\n",
       "      <td>-0.961878</td>\n",
       "      <td>-0.705189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>0.060407</td>\n",
       "      <td>0.513311</td>\n",
       "      <td>0.936505</td>\n",
       "      <td>1.006610</td>\n",
       "      <td>0.494981</td>\n",
       "      <td>0.523752</td>\n",
       "      <td>0.690706</td>\n",
       "      <td>1.139214</td>\n",
       "      <td>0.367732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231584</td>\n",
       "      <td>-0.653420</td>\n",
       "      <td>-0.375553</td>\n",
       "      <td>-0.525406</td>\n",
       "      <td>-0.577466</td>\n",
       "      <td>-0.290198</td>\n",
       "      <td>-0.706686</td>\n",
       "      <td>-0.289719</td>\n",
       "      <td>0.226173</td>\n",
       "      <td>-0.341575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>0.146047</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>0.903831</td>\n",
       "      <td>1.114801</td>\n",
       "      <td>0.560118</td>\n",
       "      <td>0.567784</td>\n",
       "      <td>0.569312</td>\n",
       "      <td>1.060930</td>\n",
       "      <td>0.367802</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291571</td>\n",
       "      <td>-0.666672</td>\n",
       "      <td>-0.421848</td>\n",
       "      <td>-0.507507</td>\n",
       "      <td>-0.323058</td>\n",
       "      <td>-0.135920</td>\n",
       "      <td>-0.595517</td>\n",
       "      <td>-0.190385</td>\n",
       "      <td>0.352865</td>\n",
       "      <td>-0.223672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>0.182291</td>\n",
       "      <td>0.524183</td>\n",
       "      <td>1.062368</td>\n",
       "      <td>1.769368</td>\n",
       "      <td>0.524241</td>\n",
       "      <td>0.670355</td>\n",
       "      <td>0.640584</td>\n",
       "      <td>1.368025</td>\n",
       "      <td>0.398322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238778</td>\n",
       "      <td>-0.655920</td>\n",
       "      <td>-0.350477</td>\n",
       "      <td>-0.702886</td>\n",
       "      <td>-0.301784</td>\n",
       "      <td>-0.476488</td>\n",
       "      <td>-0.840557</td>\n",
       "      <td>-0.463983</td>\n",
       "      <td>-0.686498</td>\n",
       "      <td>-0.473402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>0.226640</td>\n",
       "      <td>0.551584</td>\n",
       "      <td>1.093443</td>\n",
       "      <td>0.938807</td>\n",
       "      <td>0.536140</td>\n",
       "      <td>0.648923</td>\n",
       "      <td>0.666338</td>\n",
       "      <td>1.529770</td>\n",
       "      <td>0.379440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196848</td>\n",
       "      <td>-0.634120</td>\n",
       "      <td>-0.370434</td>\n",
       "      <td>-0.534597</td>\n",
       "      <td>-0.276414</td>\n",
       "      <td>-0.336610</td>\n",
       "      <td>-0.708813</td>\n",
       "      <td>-0.289107</td>\n",
       "      <td>0.201680</td>\n",
       "      <td>-0.358613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>0.095810</td>\n",
       "      <td>0.646602</td>\n",
       "      <td>1.601039</td>\n",
       "      <td>2.992876</td>\n",
       "      <td>1.051182</td>\n",
       "      <td>0.834438</td>\n",
       "      <td>0.822526</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.474598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141513</td>\n",
       "      <td>-0.614459</td>\n",
       "      <td>-0.347443</td>\n",
       "      <td>-0.480522</td>\n",
       "      <td>-0.258794</td>\n",
       "      <td>0.087169</td>\n",
       "      <td>-0.617031</td>\n",
       "      <td>-0.343468</td>\n",
       "      <td>0.218552</td>\n",
       "      <td>-0.311132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>0.076082</td>\n",
       "      <td>0.572903</td>\n",
       "      <td>1.592208</td>\n",
       "      <td>1.352963</td>\n",
       "      <td>0.647061</td>\n",
       "      <td>0.748739</td>\n",
       "      <td>0.683225</td>\n",
       "      <td>0.892138</td>\n",
       "      <td>0.472261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121144</td>\n",
       "      <td>-0.554886</td>\n",
       "      <td>-0.246593</td>\n",
       "      <td>-0.302405</td>\n",
       "      <td>-0.290815</td>\n",
       "      <td>-0.142560</td>\n",
       "      <td>-0.573636</td>\n",
       "      <td>-0.255949</td>\n",
       "      <td>0.197090</td>\n",
       "      <td>-0.293042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>0.145817</td>\n",
       "      <td>0.536628</td>\n",
       "      <td>1.878026</td>\n",
       "      <td>1.205250</td>\n",
       "      <td>0.722895</td>\n",
       "      <td>0.697033</td>\n",
       "      <td>0.758587</td>\n",
       "      <td>1.060834</td>\n",
       "      <td>0.519723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104747</td>\n",
       "      <td>-0.491800</td>\n",
       "      <td>-0.241918</td>\n",
       "      <td>-0.273605</td>\n",
       "      <td>-0.405237</td>\n",
       "      <td>-0.142150</td>\n",
       "      <td>-0.657173</td>\n",
       "      <td>-0.295666</td>\n",
       "      <td>-0.509831</td>\n",
       "      <td>-0.345833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>0.155183</td>\n",
       "      <td>0.507637</td>\n",
       "      <td>1.052264</td>\n",
       "      <td>0.862526</td>\n",
       "      <td>0.618244</td>\n",
       "      <td>0.651242</td>\n",
       "      <td>0.635841</td>\n",
       "      <td>0.976784</td>\n",
       "      <td>0.389419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238353</td>\n",
       "      <td>-0.721400</td>\n",
       "      <td>-0.406183</td>\n",
       "      <td>-0.697201</td>\n",
       "      <td>-0.550793</td>\n",
       "      <td>-0.390298</td>\n",
       "      <td>-0.831683</td>\n",
       "      <td>-0.527591</td>\n",
       "      <td>-0.789721</td>\n",
       "      <td>-0.557169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>0.155183</td>\n",
       "      <td>0.622914</td>\n",
       "      <td>1.464600</td>\n",
       "      <td>1.447367</td>\n",
       "      <td>0.663682</td>\n",
       "      <td>0.637374</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.976784</td>\n",
       "      <td>0.475883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070269</td>\n",
       "      <td>-0.504520</td>\n",
       "      <td>-0.131491</td>\n",
       "      <td>-0.260273</td>\n",
       "      <td>-0.252925</td>\n",
       "      <td>-0.158315</td>\n",
       "      <td>-0.558332</td>\n",
       "      <td>-0.215720</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.249310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  sales_10280  sales_10301  sales_10302  sales_10303  \\\n",
       "0    2012-01-01     0.064536     0.430190     0.939558     0.916459   \n",
       "1    2012-01-02     0.060407     0.513311     0.936505     1.006610   \n",
       "2    2012-01-03     0.146047     0.474314     0.903831     1.114801   \n",
       "3    2012-01-04     0.182291     0.524183     1.062368     1.769368   \n",
       "4    2012-01-05     0.226640     0.551584     1.093443     0.938807   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "361  2012-12-27     0.095810     0.646602     1.601039     2.992876   \n",
       "362  2012-12-28     0.076082     0.572903     1.592208     1.352963   \n",
       "363  2012-12-29     0.145817     0.536628     1.878026     1.205250   \n",
       "364  2012-12-30     0.155183     0.507637     1.052264     0.862526   \n",
       "365  2012-12-31     0.155183     0.622914     1.464600     1.447367   \n",
       "\n",
       "     sales_10304  sales_10305  sales_10306  sales_10307  sales_10308  ...  \\\n",
       "0       0.337124     0.541060     0.522432     0.687009     0.330482  ...   \n",
       "1       0.494981     0.523752     0.690706     1.139214     0.367732  ...   \n",
       "2       0.560118     0.567784     0.569312     1.060930     0.367802  ...   \n",
       "3       0.524241     0.670355     0.640584     1.368025     0.398322  ...   \n",
       "4       0.536140     0.648923     0.666338     1.529770     0.379440  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "361     1.051182     0.834438     0.822526     0.956280     0.474598  ...   \n",
       "362     0.647061     0.748739     0.683225     0.892138     0.472261  ...   \n",
       "363     0.722895     0.697033     0.758587     1.060834     0.519723  ...   \n",
       "364     0.618244     0.651242     0.635841     0.976784     0.389419  ...   \n",
       "365     0.663682     0.637374     0.779300     0.976784     0.475883  ...   \n",
       "\n",
       "     log_sales_11236  log_sales_11237  log_sales_11238  log_sales_11239  \\\n",
       "0          -0.318321        -0.968899        -0.600332        -0.788025   \n",
       "1          -0.231584        -0.653420        -0.375553        -0.525406   \n",
       "2          -0.291571        -0.666672        -0.421848        -0.507507   \n",
       "3          -0.238778        -0.655920        -0.350477        -0.702886   \n",
       "4          -0.196848        -0.634120        -0.370434        -0.534597   \n",
       "..               ...              ...              ...              ...   \n",
       "361        -0.141513        -0.614459        -0.347443        -0.480522   \n",
       "362        -0.121144        -0.554886        -0.246593        -0.302405   \n",
       "363        -0.104747        -0.491800        -0.241918        -0.273605   \n",
       "364        -0.238353        -0.721400        -0.406183        -0.697201   \n",
       "365        -0.070269        -0.504520        -0.131491        -0.260273   \n",
       "\n",
       "     log_sales_NYC  log_sales_Staten Island  log_sales_Queens  \\\n",
       "0        -0.864180                -0.591675         -1.043726   \n",
       "1        -0.577466                -0.290198         -0.706686   \n",
       "2        -0.323058                -0.135920         -0.595517   \n",
       "3        -0.301784                -0.476488         -0.840557   \n",
       "4        -0.276414                -0.336610         -0.708813   \n",
       "..             ...                      ...               ...   \n",
       "361      -0.258794                 0.087169         -0.617031   \n",
       "362      -0.290815                -0.142560         -0.573636   \n",
       "363      -0.405237                -0.142150         -0.657173   \n",
       "364      -0.550793                -0.390298         -0.831683   \n",
       "365      -0.252925                -0.158315         -0.558332   \n",
       "\n",
       "     log_sales_Bronx  log_sales_Manhattan  log_sales_Brooklyn  \n",
       "0          -0.675271            -0.961878           -0.705189  \n",
       "1          -0.289719             0.226173           -0.341575  \n",
       "2          -0.190385             0.352865           -0.223672  \n",
       "3          -0.463983            -0.686498           -0.473402  \n",
       "4          -0.289107             0.201680           -0.358613  \n",
       "..               ...                  ...                 ...  \n",
       "361        -0.343468             0.218552           -0.311132  \n",
       "362        -0.255949             0.197090           -0.293042  \n",
       "363        -0.295666            -0.509831           -0.345833  \n",
       "364        -0.527591            -0.789721           -0.557169  \n",
       "365        -0.215720            -0.000603           -0.249310  \n",
       "\n",
       "[366 rows x 395 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with nuisance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuisance_variables = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\nuisance_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([master_data, nuisance_variables], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values - no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.to_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\master_data_no_neg_pysent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_data.to_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\final_data_distilbert.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
