{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the regression input data\n",
    "df = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\R Regression\\otto_regression_input_no_neg_pysent.csv\", index_col =0 )\n",
    "\n",
    "# Read in sport pe data\n",
    "sports_pe = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Otto data\\Sports\\nyc_metro_sports_2012.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZIP',\n",
       " 'z_ses',\n",
       " 'county',\n",
       " 'purchase',\n",
       " 'log_purchase',\n",
       " 'HURRICANE',\n",
       " 'HOLIDAYS',\n",
       " 'NEWYEARSDAY',\n",
       " 'DAYAFTERNEWYEARS',\n",
       " 'BIRTHDAYOFMARTINLUTHERKINGJR',\n",
       " 'VALENTINESDAY',\n",
       " 'WASHINGTONSBIRTHDAY',\n",
       " 'EASTER',\n",
       " 'MEMORIALDAY',\n",
       " 'INDEPENDENCEDAY',\n",
       " 'LABORDAY',\n",
       " 'COLUMBUSDAY',\n",
       " 'VETERANSDAY',\n",
       " 'THANKSGIVING',\n",
       " 'CHRISTMASDAY',\n",
       " 'DAYAFTERCHRISTMAS',\n",
       " 'NEWYEARSEVE',\n",
       " 'FIRST_OF_MONTH',\n",
       " 'FIFTEENTH_OF_MONTH',\n",
       " 'MON',\n",
       " 'TUE',\n",
       " 'WED',\n",
       " 'THU',\n",
       " 'FRI',\n",
       " 'SAT',\n",
       " 'SUN',\n",
       " 'JAN',\n",
       " 'FEB',\n",
       " 'MAR',\n",
       " 'APR',\n",
       " 'MAY',\n",
       " 'JUN',\n",
       " 'JUL',\n",
       " 'AUG',\n",
       " 'SEP',\n",
       " 'OCT',\n",
       " 'NOV',\n",
       " 'DEC ',\n",
       " 'devils_vader_pe',\n",
       " 'giants_vader_pe',\n",
       " 'knicks_vader_pe',\n",
       " 'mets_vader_pe',\n",
       " 'yankees_vader_pe',\n",
       " 'devils_pysent_pe',\n",
       " 'giants_pysent_pe',\n",
       " 'knicks_pysent_pe',\n",
       " 'mets_pysent_pe',\n",
       " 'yankees_pysent_pe',\n",
       " 'devils_mean_no_neg_pysent',\n",
       " 'giants_mean_no_neg_pysent',\n",
       " 'knicks_mean_no_neg_pysent',\n",
       " 'mets_mean_no_neg_pysent',\n",
       " 'yankees_mean_no_neg_pysent',\n",
       " 'devils_mean_roberta',\n",
       " 'giants_mean_roberta',\n",
       " 'knicks_mean_roberta',\n",
       " 'mets_mean_roberta',\n",
       " 'yankees_mean_roberta',\n",
       " 'devils_mean_distilbert',\n",
       " 'giants_mean_distilbert',\n",
       " 'knicks_mean_distilbert',\n",
       " 'mets_mean_distilbert',\n",
       " 'yankees_mean_distilbert',\n",
       " 'devils_mean_no_neg_pysent.1',\n",
       " 'giants_mean_no_neg_pysent.1',\n",
       " 'knicks_mean_no_neg_pysent.1',\n",
       " 'mets_mean_no_neg_pysent.1',\n",
       " 'yankees_mean_no_neg_pysent.1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_RANGES = pd.date_range(datetime(2012, 1, 1), datetime(2012, 12,31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace sent scores with means on days when the team has less than 5 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_devils_processed.csv',\n",
       " 'C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_giants_processed.csv',\n",
       " 'C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_knicks_processed.csv',\n",
       " 'C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_mets_processed.csv',\n",
       " 'C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_TEST_processed.csv',\n",
       " 'C:\\\\Users\\\\clark\\\\OneDrive - University of Warwick\\\\Diss\\\\2. Analysis\\\\Data\\\\Preprocessed tweets\\\\2012_yankees_processed.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionray with the team name and date where there are less than 5 tweets\n",
    "list_of_files = glob.glob(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Preprocessed tweets\\*.csv\")\n",
    "list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a file path as input and outputs a dataframe with dates and tweet counts less than 5\n",
    "def tweets_per_day(data):\n",
    "    \n",
    "    # Load in data \n",
    "    df = pd.read_csv(data)\n",
    "    # Date ranges\n",
    "    DATE_RANGES = pd.date_range(date(2012, 1, 1), date(2012, 12,31))\n",
    "\n",
    "    # Update this with the count of tweets each day\n",
    "    tweets_per_day = pd.DataFrame(columns = [\"num_tweets\"])\n",
    "    \n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"]).dt.date\n",
    "    \n",
    "    group_df = df.groupby(\"created_at\")\n",
    "    \n",
    "    for d, day in group_df:\n",
    "        \n",
    "        # How many tweets are there on this day?\n",
    "        num_tweets = len(day)\n",
    "        # Update tweets per day\n",
    "        #print(d)\n",
    "        tweets_per_day.at[d, \"num_tweets\"] = num_tweets\n",
    "    \n",
    "    # Select the days where there are less than 5 tweets\n",
    "    less_than_five = tweets_per_day[tweets_per_day[\"num_tweets\"] <= 5]\n",
    "    \n",
    "    #print(less_than_five)\n",
    "    return less_than_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'devils': ['2012-10-30',\n",
       "  '2012-11-04',\n",
       "  '2012-11-13',\n",
       "  '2012-11-23',\n",
       "  '2012-12-18',\n",
       "  '2012-12-24'],\n",
       " 'giants': [],\n",
       " 'knicks': ['2012-06-01',\n",
       "  '2012-06-03',\n",
       "  '2012-06-09',\n",
       "  '2012-06-11',\n",
       "  '2012-07-30',\n",
       "  '2012-08-17',\n",
       "  '2012-09-01',\n",
       "  '2012-09-04',\n",
       "  '2012-09-15',\n",
       "  '2012-09-19',\n",
       "  '2012-09-23'],\n",
       " 'mets': [],\n",
       " 'yankees': ['2012-01-01']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A dictionary with the team names and respective file paths\n",
    "dict_of_files = {\"devils\": list_of_files[0],\n",
    "                \"giants\": list_of_files[1],\n",
    "                \"knicks\": list_of_files[2],\n",
    "                \"mets\": list_of_files[3],\n",
    "                \"yankees\": list_of_files[4]\n",
    "                }\n",
    "\n",
    "# Populate this dictionary with the dates upon which there are fewer than 5 tweets\n",
    "less_than_5_days = {}\n",
    "\n",
    "# Loop through each team, find dates with less than 5 and add to less_than_5_days dictionary\n",
    "for key, file in dict_of_files.items():\n",
    "    # Call tweets_per_day\n",
    "    l_t_5= tweets_per_day(file)\n",
    "    \n",
    "    # Add all the dates to the less_than_5_days dict\n",
    "    less_than_5_days[key] = [date_obj.strftime(\"%Y-%m-%d\") for date_obj in list(l_t_5.index)]\n",
    "    \n",
    "        \n",
    "less_than_5_days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ZIP', 'z_ses', 'county', 'purchase', 'log_purchase', 'HURRICANE',\n",
       "       'HOLIDAYS', 'NEWYEARSDAY', 'DAYAFTERNEWYEARS',\n",
       "       'BIRTHDAYOFMARTINLUTHERKINGJR', 'VALENTINESDAY', 'WASHINGTONSBIRTHDAY',\n",
       "       'EASTER', 'MEMORIALDAY', 'INDEPENDENCEDAY', 'LABORDAY', 'COLUMBUSDAY',\n",
       "       'VETERANSDAY', 'THANKSGIVING', 'CHRISTMASDAY', 'DAYAFTERCHRISTMAS',\n",
       "       'NEWYEARSEVE', 'FIRST_OF_MONTH', 'FIFTEENTH_OF_MONTH', 'MON', 'TUE',\n",
       "       'WED', 'THU', 'FRI', 'SAT', 'SUN', 'JAN', 'FEB', 'MAR', 'APR', 'MAY',\n",
       "       'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC ', 'devils_vader_pe',\n",
       "       'giants_vader_pe', 'knicks_vader_pe', 'mets_vader_pe',\n",
       "       'yankees_vader_pe', 'devils_pysent_pe', 'giants_pysent_pe',\n",
       "       'knicks_pysent_pe', 'mets_pysent_pe', 'yankees_pysent_pe',\n",
       "       'devils_mean_no_neg_pysent', 'giants_mean_no_neg_pysent',\n",
       "       'knicks_mean_no_neg_pysent', 'mets_mean_no_neg_pysent',\n",
       "       'yankees_mean_no_neg_pysent', 'devils_mean_roberta',\n",
       "       'giants_mean_roberta', 'knicks_mean_roberta', 'mets_mean_roberta',\n",
       "       'yankees_mean_roberta', 'devils_mean_distilbert',\n",
       "       'giants_mean_distilbert', 'knicks_mean_distilbert',\n",
       "       'mets_mean_distilbert', 'yankees_mean_distilbert',\n",
       "       'devils_mean_no_neg_pysent.1', 'giants_mean_no_neg_pysent.1',\n",
       "       'knicks_mean_no_neg_pysent.1', 'mets_mean_no_neg_pysent.1',\n",
       "       'yankees_mean_no_neg_pysent.1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that we want to replace \n",
    "replace_cols = [\"\"]\n",
    "\n",
    "# for each team\n",
    "for team, dates in less_than_5_days.items():\n",
    "    \n",
    "    # select all rows not have less than 5\n",
    "    more_than_5 = df[~df.index.isin(dates)]\n",
    "    \n",
    "    # Get a list of the sentiment scores relating to that team\n",
    "    cols_list = list(df.columns[df.columns.str.contains(team)])\n",
    "    \n",
    "    for col in cols_list:\n",
    "        # Get the mean of the more than 5 \n",
    "        mean_ = more_than_5[col].mean()\n",
    "#         print(\"The mean for \", team, \" \", col, \"is \", mean_)\n",
    "        # Replace values in original df with mean_\n",
    "        df.loc[df.index.isin(dates),col]= mean_\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate the to find the mean sales, sentiment/surprise scores for each day\n",
    "#### Create a new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each date\n",
    "date_grouped = df.groupby(df.index)\n",
    "\n",
    "for d, date_df in date_grouped:\n",
    "    \n",
    "    # Access first row of date_df\n",
    "    first_row = date_df.iloc[[0]]\n",
    "    \n",
    "    # Assign first row \n",
    "    new_df = pd.concat([new_df, first_row])\n",
    "    \n",
    "    # Update the purchase figure\n",
    "    # Find the mean of the composite_per_capita sales\n",
    "    date_mean_sales = np.mean(date_df[\"purchase\"])\n",
    "    new_df.loc[d,\"purchase\"] = date_mean_sales\n",
    "\n",
    "# drop ZIP, z_ses, county and log_purchase\n",
    "new_df = new_df.drop([\"ZIP\",\"z_ses\", \"county\", \"log_purchase\"], axis = 1)\n",
    "\n",
    "# Add a log column - natural logarithm \n",
    "new_df['log_purchase'] = log(new_df[\"purchase\"])\n",
    "\n",
    "# Sum together the vader, pysent and roberta scores\n",
    "vader_cols = list(new_df.columns[new_df.columns.str.contains(\"vader\")])\n",
    "sum_vader = new_df[vader_cols].sum(axis = 1)\n",
    "new_df[\"sum_vader\"] = sum_vader\n",
    "\n",
    "pysent_cols = list(new_df.columns[new_df.columns.str.contains(\"pysent\")])\n",
    "sum_pysent = new_df[pysent_cols].sum(axis = 1)\n",
    "new_df[\"sum_pysent\"] = sum_pysent\n",
    "\n",
    "rob_cols = list(new_df.columns[new_df.columns.str.contains(\"roberta\")])\n",
    "sum_rob = new_df[rob_cols].sum(axis = 1)\n",
    "new_df[\"sum_roberta\"] = sum_rob\n",
    "\n",
    "distilbert_cols = list(new_df.columns[new_df.columns.str.contains(\"distilbert\")])\n",
    "sum_distilbert = new_df[distilbert_cols].sum(axis = 1)\n",
    "new_df[\"sum_distilbert\"] = sum_distilbert\n",
    "\n",
    "no_neg_cols = list(new_df.columns[new_df.columns.str.contains(\"no_neg\")])\n",
    "sum_no_neg = new_df[no_neg_cols].sum(axis = 1)\n",
    "new_df[\"sum_no_neg_pysent\"] = sum_no_neg\n",
    "\n",
    "\n",
    "# Create the next day purchases data and replace the NaN value on the last day with mean sales\n",
    "new_df[\"next_day_purchase\"] = new_df[\"purchase\"].shift(-1)\n",
    "new_df[\"next_day_purchase\"].fillna(new_df[\"next_day_purchase\"].mean())\n",
    "\n",
    "# Add the log of next day purchases\n",
    "new_df[\"next_day_log_purchase\"] = log(new_df[\"next_day_purchase\"])\n",
    "\n",
    "# Add in the sports prediction errors\n",
    "new_df[\"sum_pe\"] = list(sports_pe[\"sum_pe\"])\n",
    "new_df[\"knicks_pe\"] = list(sports_pe[\"knicks_pe\"])\n",
    "new_df[\"mets_pe\"] = list(sports_pe[\"mets_pe\"])\n",
    "new_df[\"yankees_pe\"] = list(sports_pe[\"yankees_pe\"])\n",
    "new_df[\"giants_pe\"] = list(sports_pe[\"giants_pe\"])\n",
    "new_df[\"devils_pe\"] = list(sports_pe[\"devils_pe\"])\n",
    "\n",
    "# Normalise the sentiment/surprise scores\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "# new_df[\"norm_sum_vader\"] = scaler.fit_transform(np.array(new_df[\"sum_vader\"]).reshape(-1,1))\n",
    "# new_df[\"norm_sum_pysent\"] = scaler.fit_transform(np.array(new_df[\"sum_pysent\"]).reshape(-1,1))\n",
    "# new_df[\"norm_sum_roberta\"] = scaler.fit_transform(np.array(new_df[\"sum_roberta\"]).reshape(-1,1))\n",
    "\n",
    "new_df[\"norm_sum_vader\"] = stats.zscore(new_df[\"sum_vader\"])\n",
    "new_df[\"norm_sum_pysent\"] = stats.zscore(new_df[\"sum_pysent\"])\n",
    "new_df[\"norm_sum_roberta\"] = stats.zscore(new_df[\"sum_roberta\"])\n",
    "new_df[\"norm_sum_distilbert\"] = stats.zscore(new_df[\"sum_distilbert\"])\n",
    "new_df[\"norm_sum_no_neg_pysent\"] = stats.zscore(new_df[\"sum_no_neg_pysent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the new dataframe\n",
    "new_df.to_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\R Regression\\aggregate_regression_input_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows where date is sum_pe is zero\n",
    "\n",
    "non_zero_pe = new_df[new_df[\"sum_pe\"] != 0]\n",
    "\n",
    "non_zero_pe.to_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\R Regression\\aggregate_regression_input_no_zero_pe.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
