{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from _2_get_pysent_scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "from TweetNormalizer import normalizeTweet\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from pysentimiento_master.pysentimiento.preprocessing import preprocess_tweet\n",
    "from pysentimiento_master.pysentimiento.preprocessing import *\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "import urllib.request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of teams and years to iterate through for file names\n",
    "team_search_queries = {'giants': [\"2012\"],\n",
    "                      'devils': [\"2012\"],\n",
    "                      'knicks': [\"2012\"],\n",
    "                      'mets': [\"2012\"],\n",
    "                      'yankees': [\"2012\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs pre-processed data - DON'T RUN UNLESS RE-preprocessing data\n",
    "for team in team_search_queries.keys():\n",
    "    \n",
    "    year = 2012\n",
    "    # Access the correct year/team file\n",
    "    file_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\1. Twitter API\\Data\\Data - Scraped Tweets\\FINAL Tweets\"\n",
    "    file = \"\\{}_{}.csv\".format(team,year)\n",
    "\n",
    "    file_path = file_dir + file\n",
    "\n",
    "    unclean_data = pd.read_csv(file_path, index_col = \"index\")\n",
    "    unclean_data = unclean_data.drop(\"Unnamed: 0\", axis=1)\n",
    "    \n",
    "    # Pre-process tweets\n",
    "    clean_data = pysent_clean_tweet(unclean_data)\n",
    "    \n",
    "    # Save the file down in:\n",
    "    save_dir = r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Preprocessed tweets\"\n",
    "    file_name = \"\\{}_{}_processed.csv\".format(year,team)\n",
    "    clean_data.to_csv(save_dir + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the below is working notes on how to get a mood tranformer working!\n",
    "## Try transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pre-processed file\n",
    "knicks = pd.read_csv(r\"C:\\Users\\clark\\OneDrive - University of Warwick\\Diss\\2. Analysis\\Data\\Preprocessed tweets\\2012_knicks_processed.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209771195.0</td>\n",
       "      <td>1.536225e+17</td>\n",
       "      <td>@USER lmao !! Fuck it it's basketball season n...</td>\n",
       "      <td>2012-01-01 23:43:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40612777.0</td>\n",
       "      <td>1.536038e+17</td>\n",
       "      <td>Just won my first online match in nba2 k12 nyk...</td>\n",
       "      <td>2012-01-01 22:29:34+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30281791.0</td>\n",
       "      <td>1.535922e+17</td>\n",
       "      <td>So No One gonna recognize that Josh Harrelson ...</td>\n",
       "      <td>2012-01-01 21:43:37+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49532208.0</td>\n",
       "      <td>1.535882e+17</td>\n",
       "      <td>@USER @USER @USER I write about knicks &amp; we R ...</td>\n",
       "      <td>2012-01-01 21:27:44+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49532208.0</td>\n",
       "      <td>1.535850e+17</td>\n",
       "      <td>@USER How about following me back? I write abo...</td>\n",
       "      <td>2012-01-01 21:14:46+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>34889095.0</td>\n",
       "      <td>2.857623e+17</td>\n",
       "      <td>Since 1996, the overly bashed,but of all nba j...</td>\n",
       "      <td>2012-12-31 15:00:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>374801988.0</td>\n",
       "      <td>2.856106e+17</td>\n",
       "      <td>Time for me to let football go @USER @USER cow...</td>\n",
       "      <td>2012-12-31 04:57:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>44649637.0</td>\n",
       "      <td>2.856062e+17</td>\n",
       "      <td>On another note and I can now put my energy in...</td>\n",
       "      <td>2012-12-31 04:39:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>172424320.0</td>\n",
       "      <td>2.855715e+17</td>\n",
       "      <td>My nygiants lose, now all focus is my nyknicks...</td>\n",
       "      <td>2012-12-31 02:22:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>263873874.0</td>\n",
       "      <td>2.855543e+17</td>\n",
       "      <td>Iman shumpert your about to be my header daddy...</td>\n",
       "      <td>2012-12-31 01:13:48+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23482 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id      tweet_id  \\\n",
       "index                              \n",
       "0      209771195.0  1.536225e+17   \n",
       "1       40612777.0  1.536038e+17   \n",
       "2       30281791.0  1.535922e+17   \n",
       "3       49532208.0  1.535882e+17   \n",
       "4       49532208.0  1.535850e+17   \n",
       "...            ...           ...   \n",
       "2870    34889095.0  2.857623e+17   \n",
       "2871   374801988.0  2.856106e+17   \n",
       "2872    44649637.0  2.856062e+17   \n",
       "2873   172424320.0  2.855715e+17   \n",
       "2874   263873874.0  2.855543e+17   \n",
       "\n",
       "                                                    text  \\\n",
       "index                                                      \n",
       "0      @USER lmao !! Fuck it it's basketball season n...   \n",
       "1      Just won my first online match in nba2 k12 nyk...   \n",
       "2      So No One gonna recognize that Josh Harrelson ...   \n",
       "3      @USER @USER @USER I write about knicks & we R ...   \n",
       "4      @USER How about following me back? I write abo...   \n",
       "...                                                  ...   \n",
       "2870   Since 1996, the overly bashed,but of all nba j...   \n",
       "2871   Time for me to let football go @USER @USER cow...   \n",
       "2872   On another note and I can now put my energy in...   \n",
       "2873   My nygiants lose, now all focus is my nyknicks...   \n",
       "2874   Iman shumpert your about to be my header daddy...   \n",
       "\n",
       "                      created_at  \n",
       "index                             \n",
       "0      2012-01-01 23:43:53+00:00  \n",
       "1      2012-01-01 22:29:34+00:00  \n",
       "2      2012-01-01 21:43:37+00:00  \n",
       "3      2012-01-01 21:27:44+00:00  \n",
       "4      2012-01-01 21:14:46+00:00  \n",
       "...                          ...  \n",
       "2870   2012-12-31 15:00:11+00:00  \n",
       "2871   2012-12-31 04:57:32+00:00  \n",
       "2872   2012-12-31 04:39:56+00:00  \n",
       "2873   2012-12-31 02:22:00+00:00  \n",
       "2874   2012-12-31 01:13:48+00:00  \n",
       "\n",
       "[23482 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at knicks\n",
    "knicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe44492d07e54088a776f8c52c1c1ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f581a369d64c7caf5289b4af7c82cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef120a3933046d8bf89572e262b7614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf76a2e98a248c599bd3a74ebb2b217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f70f39f22b04650bb80540b5acb91d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the sentiment analyser pipeline\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "\n",
    "# Tryy wit \"text-classification\"\n",
    "classifier = pipeline(\"text-classification\", model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just won my first online match in nba2 k12 nyknicks over the Heat.. @USER dropped 26...im 1-0...now ill retire! Perfect record!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet = knicks[\"text\"].iloc[1]\n",
    "test_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = classifier(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'optimism', 'score': 0.87562495470047}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) joy 0.8756\n",
      "2) optimism 0.0655\n",
      "3) anger 0.045\n",
      "4) sadness 0.014\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(test_tweet, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Celebrating my promotion ðŸ˜Ž\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RobertaForSequenceClassification' is not supported for token-classification. Supported models are ['QDQBertForTokenClassification', 'FNetForTokenClassification', 'LayoutLMv2ForTokenClassification', 'RemBertForTokenClassification', 'CanineForTokenClassification', 'RoFormerForTokenClassification', 'BigBirdForTokenClassification', 'ConvBertForTokenClassification', 'LayoutLMForTokenClassification', 'DistilBertForTokenClassification', 'CamembertForTokenClassification', 'FlaubertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'LongformerForTokenClassification', 'RobertaForTokenClassification', 'SqueezeBertForTokenClassification', 'BertForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'XLNetForTokenClassification', 'AlbertForTokenClassification', 'ElectraForTokenClassification', 'FunnelForTokenClassification', 'MPNetForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification'].\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForSequenceClassification.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")\n",
    "# Tryy wit \"text-classification\"\n",
    "classifier = pipeline(\"text-classification\", model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysentimiento.metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e96891727521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpysentimiento\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_analyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_analyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"emotion\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pysentimiento\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_analyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pysentimiento.metrics'"
     ]
    }
   ],
   "source": [
    "# from pysentimiento import create_analyzer\n",
    "# analyzer = create_analyzer(task=\"emotion\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3672e780c7424a96c465bf4ba63386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be75ca9eb0c84a32ac890933de49bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eef52bb26c48fcb0890f912421c8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2abd94800c4fd0902c0bb04f7c46f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607bb2e514b94d18ac7f9658c2cb4991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b66ef6df7aa4f2e87732555ade11114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6f7821b92a4e188911f86fef4db1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "# Tryy wit \"text-classification\"\n",
    "classifier = pipeline(\"text-classification\", model = model, tokenizer = tokenizer, return_all_scores=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5532079339027405"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = classifier(test_tweet)\n",
    "test_output[0][6]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCION THAT TAKES A DATAFRAME AS INPUT AND OUTPUTS IT WITH LABELS AND SCORES\n",
    "def get_mood(df):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    " \n",
    "    classifier = pipeline(\"text-classification\", model = model, tokenizer = tokenizer, return_all_scores=True)\n",
    "    \n",
    "    # Take a list of the tweets and find the sentiment and score of each \n",
    "    tweets = list(df[\"text\"])\n",
    "    \n",
    "    labels = []\n",
    "    scores = []\n",
    "    \n",
    "    for sent in tqdm(tweets):\n",
    "        \n",
    "        try:\n",
    "            results = classifier(sent)\n",
    "            #Assign label to label\n",
    "            #df.at[i,\"label\"] = results[0][0][\"label\"]\n",
    "            #df.at[i,\"score\"] = results[0][0][\"score\"]\n",
    "            \n",
    "            labels.append(results[0][6][\"label\"])\n",
    "            scores.append(results[0][6]['score'])\n",
    "        # Get the exception to print out the error and for which team/time then pass to next month\n",
    "        except:\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "            labels.append(\"DELETE\")\n",
    "            scores.append(100000)\n",
    "            print(\"The tweet was: \",sent)\n",
    "            pass\n",
    "\n",
    "    # Create an empty col for label and score\n",
    "    df = df.assign(label = labels)\n",
    "    df = df.assign(score = scores)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                              | 9350/23482 [44:41<37:42,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet was:  Steve Novak just makes my heart sing in a totally platonic way every time he hits a three and then does the Double Check. nyknicks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 9382/23482 [44:48<47:12,  4.98it/s]"
     ]
    }
   ],
   "source": [
    "test_mood = get_mood(knicks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
