{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets (2013 - Only Giants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing/ Downloading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages - uncomment if required\n",
    "\n",
    "#pip install tweepy\n",
    "#pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tweepy\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from calendar import monthrange\n",
    "import os\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "#### - Use a tweepy search_all_tweets function to find tweets that contains #Giants (to be replaced with whichever sports team we are searching for).\n",
    "#### The serch query uses:   is not a retweet (-is:retweet), is in English (lang:en), the tweet does not include links (-has:links) and is geo-tagged within 10km of the longitudinal/latitudinal coordinates of the Empire State building (40.7301366,-74.1831972) (point_radius:40.7301366,-74.1831972,10km).\n",
    "#### - This search function will be run for each day. This is because the end_time and start_time of the search function uses returns the most recent x tweets in relation to end_time  - therefore, to get equal amount of tweets spread over the year - need to specify the start_time and end_time for each day. This will be run iteratively - stored in the dictionary 'daily_start_end_time'.\n",
    "#### - The information from each month will be saved down in a csv file. Therefore - for 2012 and 2013 there will be 24 file outputs. The reason for doing this is to reduce the risk of an error interupting the code and having to start from the beginning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Authentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT ACADEMIC API BEARER TOKEN HERE\n",
    "bearer_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A rate limit shows an error if you've made too many queries - Tweepy will either stop your code\n",
    "# or wait until you have more requests. Using True waits til you can have more queries. \n",
    "client = tweepy.Client(bearer_token = bearer_token, \n",
    "                       wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### daily_start_end_time': Create a list of the start_time and end_time for each day in each month (in 2012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store the name of the month, the numeric reference (e.g. jan = 1, feb = 2) and \n",
    "# the num of days in the dictionary months \n",
    "month_names = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "\n",
    "# This dict will contain the start and end time for each day in each month\n",
    "daily_start_end_time = {'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[],'11':[],'12':[]}\n",
    "\n",
    "# Update months \n",
    "for month, month_str in enumerate(month_names):\n",
    "    \n",
    "    # Add 1 to month (as the first index is 0)\n",
    "    month = month + 1 \n",
    "    # The number of days in the the month (add 1 because the first index is 0)\n",
    "    num_days = monthrange(2013, month)[1]\n",
    "    # print(\"Number of days in \", month_str, \" is \" ,str(num_days))\n",
    "    \n",
    "    # Loop through each day of the month and add the start_time and end_time to the daily_end_start_time dictionary\n",
    "    for day in range(1,num_days + 1):\n",
    "        \n",
    "        \n",
    "        # The start_time \n",
    "        start_time = datetime.datetime(2013, month, day, 0, 0, 0, 0, datetime.timezone.utc)\n",
    "\n",
    "        # The end_time for the month - using num_days\n",
    "        end_time = datetime.datetime(2013, month, day, 23, 59, 59, 0, datetime.timezone.utc)\n",
    "        \n",
    "        # Append to the month a list containing the start and end time for the day\n",
    "        daily_start_end_time[month_str].append([start_time, end_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dictionary 'daily_start_end_time' is correct\n",
    "# daily_start_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with each of the New York sports teams in and their twitter query\n",
    "team_search_queries = {'giants': '#Giants -is:retweet -has:links lang:en'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main search function:\n",
    "#### - Currently looks up 200 tweets per day (Controlled by 'max_results' and 'limit') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to see if tweets for the month/team have already been scraped:\n",
    "\n",
    "#### The if statement will check to see whether a csv file exists in the working directory for a month/team (e.g. 1_giants_tweets_2012). If it already exists then the code will not run for that month/team and will move until it gets to a month for which the data has not been scraped yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with errors when there are no tweets on that day:\n",
    "\n",
    "#### try, except: This aims to resolve the error that is produced when there is no tweet data for a specific date. The error if thrown out when the code tries to the API output (response) into user friendly format - but cannot when there is no data to sort. If the data does not exist a KeyError is raised. Now, if this happens the code will move onto the next day, using 'continue'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each team\n",
    "for team, query in team_search_queries.items():\n",
    "    # loop through each month and each day and export results to a csv\n",
    "    # month is the key to the dict, and times is a list of lists - in each is a start and end time\n",
    "    for month, times in daily_start_end_time.items():\n",
    "        \n",
    "        ### Check to see if the output csv file exists in the working directory already \n",
    "        \n",
    "        # Get the current file working directory\n",
    "        working_directory = os.getcwd()\n",
    "        csv_file_name = \"\\\\{}_{}_2013_tweets.csv\".format(month, team)\n",
    "        file_path = working_directory + csv_file_name\n",
    "        \n",
    "        # does_it_exist is True if there is a file path that exists\n",
    "        does_it_exist = exists(file_path)\n",
    "        \n",
    "        # If the file already exists then move to the next month\n",
    "        if does_it_exist == True:\n",
    "            print(\"The file for month: {}, team: {} already exists\".format(month, team))\n",
    "            continue\n",
    "        \n",
    "        # If the file does not already exist - then run the API search\n",
    "        else:\n",
    "            team_tweets = []\n",
    "\n",
    "            # Loop through each start_time and end_time\n",
    "            for day in times:\n",
    "                # NOTE: the query is case insensitive - so finds both #Giants as well as #giants - adding more noise \n",
    "                for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                             # Tweet contains '#Giants, is not a retweet, does not have links, is in english\n",
    "                                             query = query,\n",
    "                                             # user_fields info is stored as features of the object in the team_tweets .includes['users'] \n",
    "                                             user_fields = ['username', 'location'],\n",
    "                                             tweet_fields = ['created_at', 'text'],\n",
    "                                             expansions = 'author_id',\n",
    "                                             # start and end time come from the daily_start_end_time dictionary\n",
    "                                             start_time = day[0],\n",
    "                                             end_time = day[1],\n",
    "                                            max_results= 20, \n",
    "                                            limit = 10):\n",
    "\n",
    "\n",
    "                    # API only allows one request per second - so wait for 1 sec \n",
    "                    time.sleep(1)\n",
    "                    team_tweets.append(response)\n",
    "\n",
    "            ### Tranform the search results for each month into a usable format and save as csv \n",
    "            result = []\n",
    "            user_dict = {}\n",
    "            # Loop through each response object\n",
    "            for response in team_tweets:\n",
    "                # Try to organise the tweets into user friendly format. A KeyError is raised if there is no data- in which case\n",
    "                # continue\n",
    "                try:\n",
    "\n",
    "                    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "                    for user in response.includes['users']:\n",
    "                        user_dict[user.id] = {'username': user.username, \n",
    "                                              'location': user.location\n",
    "                                             }\n",
    "\n",
    "\n",
    "                    for tweet in response.data:\n",
    "                        # For each tweet, find the author's information\n",
    "                        # author_info = user_dict[tweet.author_id]\n",
    "                        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "                        result.append({'author_id': tweet.author_id, \n",
    "                                       # 'username': author_info['username'],\n",
    "                                       'tweet_id': tweet.id,\n",
    "                                       'text': tweet.text,\n",
    "                                       'created_at': tweet.created_at})\n",
    "                \n",
    "                # If a KeyError is raised then move on to the next day\n",
    "                except KeyError:\n",
    "                        print(\"Except 'response' clause executed\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "            # Change this list of dictionaries into a dataframe\n",
    "            monthly_df = pd.DataFrame(result)\n",
    "\n",
    "            # Save the dataframe for the month as a csv file\n",
    "            monthly_df.to_csv(\"{}_{}_2013_tweets.csv\".format(month, team), sep = ',', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
